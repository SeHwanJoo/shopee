{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wl4RDmoeFg1N","executionInfo":{"status":"ok","timestamp":1620154313789,"user_tz":-540,"elapsed":21116,"user":{"displayName":"주세환","photoUrl":"","userId":"06218339655139120809"}},"outputId":"8baac9d8-f5bc-4be8-cbac-b14eb9037878"},"source":["import os\n","from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvD8R0cnH7VJ","executionInfo":{"status":"ok","timestamp":1620154317589,"user_tz":-540,"elapsed":24897,"user":{"displayName":"주세환","photoUrl":"","userId":"06218339655139120809"}},"outputId":"c67b2610-1a0c-41ed-8493-5c14567d82cc"},"source":["!pip install sentencepiece"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 5.9MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YOaB7mVSFz0x","executionInfo":{"status":"ok","timestamp":1620154324041,"user_tz":-540,"elapsed":31340,"user":{"displayName":"주세환","photoUrl":"","userId":"06218339655139120809"}}},"source":["import sys\n","from shutil import copyfile\n","import gc\n","import re\n","import os\n","import numpy as np\n","import pandas as pd\n","import random\n","import math\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import warnings\n","warnings.simplefilter('ignore')\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras import backend as K\n","# copyfile(src = \"../input/shopee-tokenization/tokenization.py\", dst = \"../working/tokenization.py\")\n","sys.path.append('/content/drive/MyDrive/compete/shopee/shopee-tokenization/')\n","import tokenization\n","import tensorflow_hub as hub"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOSE_tt7GXtx","executionInfo":{"status":"ok","timestamp":1620154324043,"user_tz":-540,"elapsed":31335,"user":{"displayName":"주세환","photoUrl":"","userId":"06218339655139120809"}}},"source":["# Configuration\n","EPOCHS = 10\n","BATCH_SIZE = 8\n","# Seed\n","SEED = 123\n","# Verbosity\n","VERBOSE = 1\n","LR = 1e-3\n","LENGTHS = [64, 128, 256]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNYi-7gFoCts","executionInfo":{"status":"ok","timestamp":1620154324043,"user_tz":-540,"elapsed":31329,"user":{"displayName":"주세환","photoUrl":"","userId":"06218339655139120809"}}},"source":["import re\n","\n","import tensorflow as tf\n","\n","\n","class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    \"\"\"Applys a warmup schedule on a given learning rate decay schedule.\"\"\"\n","\n","    def __init__(\n","            self,\n","            initial_learning_rate,\n","            decay_schedule_fn,\n","            warmup_steps,\n","            power=1.0,\n","            name=None):\n","        super(WarmUp, self).__init__()\n","        self.initial_learning_rate = initial_learning_rate\n","        self.warmup_steps = warmup_steps\n","        self.power = power\n","        self.decay_schedule_fn = decay_schedule_fn\n","        self.name = name\n","\n","    def __call__(self, step):\n","        with tf.name_scope(self.name or 'WarmUp') as name:\n","            # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n","            # learning rate will be `global_step/num_warmup_steps * init_lr`.\n","            global_step_float = tf.cast(step, tf.float32)\n","            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n","            warmup_percent_done = global_step_float / warmup_steps_float\n","            warmup_learning_rate = (\n","                    self.initial_learning_rate *\n","                    tf.math.pow(warmup_percent_done, self.power))\n","            return tf.cond(global_step_float < warmup_steps_float,\n","                           lambda: warmup_learning_rate,\n","                           lambda: self.decay_schedule_fn(step),\n","                           name=name)\n","\n","    def get_config(self):\n","        return {\n","            'initial_learning_rate': self.initial_learning_rate,\n","            'decay_schedule_fn': self.decay_schedule_fn,\n","            'warmup_steps': self.warmup_steps,\n","            'power': self.power,\n","            'name': self.name\n","        }\n","\n","\n","def create_optimizer(init_lr, num_train_steps, num_warmup_steps):\n","    \"\"\"Creates an optimizer with learning rate schedule.\"\"\"\n","    # Implements linear decay of the learning rate.\n","    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n","        initial_learning_rate=init_lr,\n","        decay_steps=num_train_steps,\n","        end_learning_rate=0.0)\n","    if num_warmup_steps:\n","        learning_rate_fn = WarmUp(initial_learning_rate=init_lr,\n","                                  decay_schedule_fn=learning_rate_fn,\n","                                  warmup_steps=num_warmup_steps)\n","    optimizer = AdamWeightDecay(\n","        learning_rate=learning_rate_fn,\n","        weight_decay_rate=0.01,\n","        beta_1=0.9,\n","        beta_2=0.999,\n","        epsilon=1e-6,\n","        exclude_from_weight_decay=['layer_norm', 'bias'])\n","    return optimizer\n","\n","\n","class AdamWeightDecay(tf.keras.optimizers.Adam):\n","    \"\"\"Adam enables L2 weight decay and clip_by_global_norm on gradients.\n","\n","  Just adding the square of the weights to the loss function is *not* the\n","  correct way of using L2 regularization/weight decay with Adam, since that will\n","  interact with the m and v parameters in strange ways.\n","\n","  Instead we want ot decay the weights in a manner that doesn't interact with\n","  the m/v parameters. This is equivalent to adding the square of the weights to\n","  the loss with plain (non-momentum) SGD.\n","  \"\"\"\n","\n","    def __init__(self,\n","                 learning_rate=0.001,\n","                 beta_1=0.9,\n","                 beta_2=0.999,\n","                 epsilon=1e-7,\n","                 amsgrad=False,\n","                 weight_decay_rate=0.0,\n","                 include_in_weight_decay=None,\n","                 exclude_from_weight_decay=None,\n","                 name='AdamWeightDecay',\n","                 **kwargs):\n","        super(AdamWeightDecay, self).__init__(\n","            learning_rate, beta_1, beta_2, epsilon, amsgrad, name, **kwargs)\n","        self.weight_decay_rate = weight_decay_rate\n","        self._include_in_weight_decay = include_in_weight_decay\n","        self._exclude_from_weight_decay = exclude_from_weight_decay\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        \"\"\"Creates an optimizer from its config with WarmUp custom object.\"\"\"\n","        custom_objects = {'WarmUp': WarmUp}\n","        return super(AdamWeightDecay, cls).from_config(\n","            config, custom_objects=custom_objects)\n","\n","    def _prepare_local(self, var_device, var_dtype, apply_state):\n","        super(AdamWeightDecay, self)._prepare_local(var_device, var_dtype,\n","                                                    apply_state)\n","        apply_state['weight_decay_rate'] = tf.constant(\n","            self.weight_decay_rate, name='adam_weight_decay_rate')\n","\n","    def _decay_weights_op(self, var, learning_rate, apply_state):\n","        do_decay = self._do_use_weight_decay(var.name)\n","        if do_decay:\n","            return var.assign_sub(\n","                learning_rate * var *\n","                apply_state['weight_decay_rate'],\n","                use_locking=self._use_locking)\n","        return tf.no_op()\n","\n","    def apply_gradients(self, grads_and_vars, name=None):\n","        grads, tvars = list(zip(*grads_and_vars))\n","        (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n","        return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars))\n","\n","    def _get_lr(self, var_device, var_dtype, apply_state):\n","        \"\"\"Retrieves the learning rate with the given state.\"\"\"\n","        if apply_state is None:\n","            return self._decayed_lr_t[var_dtype], {}\n","\n","        apply_state = apply_state or {}\n","        coefficients = apply_state.get((var_device, var_dtype))\n","        if coefficients is None:\n","            coefficients = self._fallback_apply_state(var_device, var_dtype)\n","            apply_state[(var_device, var_dtype)] = coefficients\n","\n","        return coefficients['lr_t'], dict(apply_state=apply_state)\n","\n","    def _resource_apply_dense(self, grad, var, apply_state=None):\n","        lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state)\n","        decay = self._decay_weights_op(var, lr_t, apply_state)\n","        with tf.control_dependencies([decay]):\n","            return super(AdamWeightDecay, self)._resource_apply_dense(\n","                grad, var, **kwargs)\n","\n","    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n","        lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state)\n","        decay = self._decay_weights_op(var, lr_t, apply_state)\n","        with tf.control_dependencies([decay]):\n","            return super(AdamWeightDecay, self)._resource_apply_sparse(\n","                grad, var, indices, **kwargs)\n","\n","    def get_config(self):\n","        config = super(AdamWeightDecay, self).get_config()\n","        config.update({\n","            'weight_decay_rate': self.weight_decay_rate,\n","        })\n","        return config\n","\n","    def _do_use_weight_decay(self, param_name):\n","        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n","        if self.weight_decay_rate == 0:\n","            return False\n","\n","        if self._include_in_weight_decay:\n","            for r in self._include_in_weight_decay:\n","                if re.search(r, param_name) is not None:\n","                    return True\n","\n","        if self._exclude_from_weight_decay:\n","            for r in self._exclude_from_weight_decay:\n","                if re.search(r, param_name) is not None:\n","                    return False\n","        return True"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hhs4rJSmHO7r","executionInfo":{"status":"ok","timestamp":1620154324728,"user_tz":-540,"elapsed":32007,"user":{"displayName":"주세환","photoUrl":"","userId":"06218339655139120809"}}},"source":["# Function to seed everything\n","def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    tf.random.set_seed(seed)\n","    \n","def read_and_preprocess():\n","    df = pd.read_csv('/content/drive/MyDrive/compete/shopee/shopee-product-matching_/train.csv')\n","    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n","    df['matches'] = df['label_group'].map(tmp)\n","    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n","    encoder = LabelEncoder()\n","    df['label_group'] = encoder.fit_transform(df['label_group'])\n","    N_CLASSES = df['label_group'].nunique()\n","    print(f'We have {N_CLASSES} classes')\n","    x_train, x_val, y_train, y_val = train_test_split(df[['title']], df['label_group'], shuffle = True, stratify = df['label_group'], random_state = SEED, test_size = 0.33)\n","    return df, N_CLASSES, x_train, x_val, y_train, y_val\n","\n","# Return tokens, masks and segments from a text array or series\n","def bert_encode(texts, tokenizer, max_len=512):\n","    all_tokens = []\n","    all_masks = []\n","    all_segments = []\n","    \n","    for text in texts:\n","        text = tokenizer.tokenize(text)\n","            \n","        text = text[:max_len-2]\n","        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n","        pad_len = max_len - len(input_sequence)\n","        \n","        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n","        tokens += [0] * pad_len\n","        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n","        segment_ids = [0] * max_len\n","        \n","        all_tokens.append(tokens)\n","        all_masks.append(pad_masks)\n","        all_segments.append(segment_ids)\n","    \n","    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n","\n","\n","# Arcmarginproduct class keras layer\n","class ArcMarginProduct(tf.keras.layers.Layer):\n","    '''\n","    Implements large margin arc distance.\n","\n","    Reference:\n","        https://arxiv.org/pdf/1801.07698.pdf\n","        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n","            blob/master/src/modeling/metric_learning.py\n","    '''\n","    def __init__(self, n_classes, s=48, m=0.50, easy_margin=False,\n","                 ls_eps=0.0, **kwargs):\n","\n","        super(ArcMarginProduct, self).__init__(**kwargs)\n","\n","        self.n_classes = n_classes\n","        self.s = s\n","        self.m = m\n","        self.ls_eps = ls_eps\n","        self.easy_margin = easy_margin\n","        self.cos_m = tf.math.cos(m)\n","        self.sin_m = tf.math.sin(m)\n","        self.th = tf.math.cos(math.pi - m)\n","        self.mm = tf.math.sin(math.pi - m) * m\n","\n","    def get_config(self):\n","\n","        config = super().get_config().copy()\n","        config.update({\n","            'n_classes': self.n_classes,\n","            's': self.s,\n","            'm': self.m,\n","            'ls_eps': self.ls_eps,\n","            'easy_margin': self.easy_margin,\n","        })\n","        return config\n","\n","    def build(self, input_shape):\n","        super(ArcMarginProduct, self).build(input_shape[0])\n","\n","        self.W = self.add_weight(\n","            name='W',\n","            shape=(int(input_shape[0][-1]), self.n_classes),\n","            initializer='glorot_uniform',\n","            dtype='float32',\n","            trainable=True,\n","            regularizer=None)\n","\n","    def call(self, inputs):\n","        X, y = inputs\n","        y = tf.cast(y, dtype=tf.int32)\n","        cosine = tf.matmul(\n","            tf.math.l2_normalize(X, axis=1),\n","            tf.math.l2_normalize(self.W, axis=0)\n","        )\n","        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n","        phi = cosine * self.cos_m - sine * self.sin_m\n","        if self.easy_margin:\n","            phi = tf.where(cosine > 0, phi, cosine)\n","        else:\n","            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n","        one_hot = tf.cast(\n","            tf.one_hot(y, depth=self.n_classes),\n","            dtype=cosine.dtype\n","        )\n","        if self.ls_eps > 0:\n","            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n","\n","        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n","        output *= self.s\n","        return output\n","\n","# Function to build bert model\n","def build_bert_model(bert_layer, data_len, max_len = 512):\n","    \n","    margin = ArcMarginProduct(\n","            n_classes = N_CLASSES, \n","            s = 48, \n","            m = 0.5, \n","            name='head/arc_margin', \n","            dtype='float32'\n","            )\n","    \n","    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","    label = tf.keras.layers.Input(shape = (), name = 'label')\n","\n","    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    clf_output = sequence_output[:, 0, :]\n","    x = margin([clf_output, label])\n","    output = tf.keras.layers.Softmax(dtype='float32')(x)\n","    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n","    model.compile(optimizer = create_optimizer(LR, data_len * EPOCHS, data_len),\n","                  loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n","                  metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n","    return model\n","\n","def load_train_and_evaluate(x_train, x_val, y_train, y_val, max_len=512):\n","    seed_everything(SEED)\n","    # Load BERT from the Tensorflow Hub\n","    module_url = \"/content/drive/MyDrive/compete/shopee/shopee-tokenization/bert_en_uncased_L-24_H-1024_A-16_1\"\n","    bert_layer = hub.KerasLayer(module_url, trainable = True)\n","    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n","    x_train = bert_encode(x_train['title'].values, tokenizer, max_len = max_len)\n","    x_val = bert_encode(x_val['title'].values, tokenizer, max_len = max_len)\n","    y_train = y_train.values\n","    y_val = y_val.values\n","    # Add targets to train and val\n","    x_train = (x_train[0], x_train[1], x_train[2], y_train)\n","    x_val = (x_val[0], x_val[1], x_val[2], y_val)\n","    bert_model = build_bert_model(bert_layer, data_len=math.ceil(len(x_train[0])/BATCH_SIZE), max_len = max_len)\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/compete/shopee/bert_weight/Bert_' + str(max_len) + '_{epoch}.h5', \n","                                                    monitor = 'val_loss', \n","                                                    verbose = VERBOSE, \n","                                                    save_best_only = False,\n","                                                    save_weights_only = True, \n","                                                    mode = 'min',\n","                                                    save_freq = 5 * math.ceil(len(x_train[0])/BATCH_SIZE))\n","    \n","    \n","    history = bert_model.fit(x_train, y_train,\n","                            validation_data = (x_val, y_val),\n","                            epochs = EPOCHS, \n","                            callbacks = [checkpoint],\n","                            batch_size = BATCH_SIZE,\n","                            verbose = VERBOSE)\n","    \n","    del bert_model\n","    gc.collect()\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsR-KHFU4xJS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a059254-33e4-4fea-a93f-a5359067f891"},"source":["df, N_CLASSES, x_train, x_val, y_train, y_val = read_and_preprocess()\n","for length in LENGTHS:\n","    print('-' * 50 + str(length) + '-' * 50)\n","    load_train_and_evaluate(x_train, x_val, y_train, y_val, max_len = length)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We have 11014 classes\n","--------------------------------------------------64--------------------------------------------------\n","Epoch 1/10\n","2869/2869 [==============================] - 547s 178ms/step - loss: 31.3757 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 21.0175 - val_sparse_categorical_accuracy: 0.0000e+00\n","Epoch 2/10\n","2869/2869 [==============================] - 507s 177ms/step - loss: 20.8752 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 20.7805 - val_sparse_categorical_accuracy: 0.0000e+00\n","Epoch 3/10\n","2869/2869 [==============================] - 507s 177ms/step - loss: 20.7779 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 20.7511 - val_sparse_categorical_accuracy: 0.0000e+00\n","Epoch 4/10\n"," 172/2869 [>.............................] - ETA: 7:03 - loss: 20.6593 - sparse_categorical_accuracy: 0.0000e+00"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BibQywvtr26P"},"source":[""],"execution_count":null,"outputs":[]}]}